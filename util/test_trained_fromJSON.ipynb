{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d6eeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "# From model script, workaround to serialize embed layer\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class L2Normalization(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.math.l2_normalize(inputs, axis=1)\n",
    "\n",
    "\n",
    "# Load the trained encoder\n",
    "encoder = load_model(\"../gesture_encoder_model.keras\", safe_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0ad577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raw_to_dataset import load_gesture_xyz\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from raw_to_dataset import sample_points_fixed, load_gesture_xyz\n",
    "\n",
    "# Computes the distance matrix for a list of points\n",
    "def compute_distance_matrix(points):\n",
    "    if (len(points) != 128):\n",
    "        points = sample_points_fixed(points, N=128)\n",
    "\n",
    "    \"\"\"\n",
    "    points: (num_points, 3)\n",
    "    returns: (num_points, num_points) distance matrix\n",
    "    \"\"\"\n",
    "    points = np.asarray(points, dtype=np.float32)\n",
    "    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "    dist_matrix = np.linalg.norm(diff, axis=-1)\n",
    "    return dist_matrix\n",
    "\n",
    "# Gets an embedding for a gesture (represented by a list of xyz points)\n",
    "def get_embedding(gesture_points):\n",
    "    \"\"\"\n",
    "    gesture_points: (128, 3) numpy array of a gesture\n",
    "    returns: (EMBED_DIM,) embedding\n",
    "    \"\"\"\n",
    "    dist_matrix = compute_distance_matrix(gesture_points)\n",
    "    dist_matrix = np.expand_dims(dist_matrix, axis=0)  # batch dimension\n",
    "    embedding = encoder.predict(dist_matrix, verbose=0)\n",
    "    return embedding[0]\n",
    "\n",
    "\n",
    "\n",
    "# Gets a comparison score between 2 gestures\n",
    "def compare_gestures(gesture1, gesture2):\n",
    "    \"\"\"\n",
    "    gesture1: (128, 3) numpy array of a gesture\n",
    "    gesture2: (128, 3) numpy array of a gesture\n",
    "    \"\"\"\n",
    "    emb1 = get_embedding(gesture1)\n",
    "    emb2 = get_embedding(gesture2)\n",
    "\n",
    "    score = np.linalg.norm(emb1 - emb2)\n",
    "    return score\n",
    "\n",
    "# Gets comparison score between 2 gestures, given filepaths to .json representations\n",
    "def compare_gestures_filepath(filepath1, filepath2):\n",
    "    \"\"\"\n",
    "    filepath1, filepath2: string filepaths to .json format of gestures, can be of any size. See sample data.\n",
    "    \"\"\"\n",
    "    points1 = load_gesture_xyz(filepath1)\n",
    "    points2 = load_gesture_xyz(filepath2)\n",
    "    return compare_gestures(points1, points2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14902682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apt_verticalL score: 0.3138\n",
      "apt1 score: 0.4852\n",
      "concert score: 0.2145\n",
      "house1 score: 0.6727\n"
     ]
    }
   ],
   "source": [
    "# Change paths to view score\n",
    "gest_filepath = \"sample_data/concert_sample/concert_sample_default.json\"\n",
    "\n",
    "apt_verticalL_score = compare_gestures_filepath(gest_filepath, \"sample_data/apt_verticalL_sample/apt_verticalL_sample_default2.json\")\n",
    "apt1_score = compare_gestures_filepath(gest_filepath, \"sample_data/apt1_sample/apt1_sample_1.json\")\n",
    "concert_score = compare_gestures_filepath(gest_filepath, \"sample_data/concert_sample/concert_sample_default2.json\")\n",
    "house1_score = compare_gestures_filepath(gest_filepath, \"sample_data/house1_sample/house1_sample_default2.json\")\n",
    "\n",
    "print(f\"apt_verticalL score: {apt_verticalL_score:.4f}\")\n",
    "print(f\"apt1 score: {apt1_score:.4f}\")\n",
    "print(f\"concert score: {concert_score:.4f}\")\n",
    "print(f\"house1 score: {house1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f182ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apt1_sample: 0.5412\n",
      "apt_verticalL_sample: 0.2757\n",
      "concert_sample: 0.5281\n",
      "house1_sample: 1.0279\n"
     ]
    }
   ],
   "source": [
    "# Compare to all .json gestures in sample_data, \n",
    "#  - allows multiple representations of each gesture and takes the average score\n",
    "base_dir = \"sample_data\"\n",
    "reference_filepath = \"sample_data/concert_sample/concert_sample_default.json\"\n",
    "# reference_filepath = \"sample_data/apt_verticalL_sample/apt_verticalL_sample_default3.json\"\n",
    "\n",
    "avg_scores = {} # Dictionary to store average score for each gesture\n",
    "\n",
    "# Find average of comparisons to each gesture class' examples\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Get score of each gesture\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        gesture_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Skip the reference file itself in case using sample data as example\n",
    "        if gesture_path == reference_filepath:\n",
    "            continue\n",
    "\n",
    "        dist = compare_gestures_filepath(reference_filepath, gesture_path)\n",
    "        scores.append(dist)\n",
    "\n",
    "    if scores:\n",
    "        avg_scores[folder] = sum(scores) / len(scores)\n",
    "    else:\n",
    "        avg_scores[folder] = None  # no valid files\n",
    "\n",
    "# Print results\n",
    "for folder, avg in avg_scores.items():\n",
    "    if avg is None:\n",
    "        print(f\"{folder}: (no gestures found)\")\n",
    "    else:\n",
    "        print(f\"{folder}: {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f9bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XR_Gesture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
