{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c587bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Input\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d502d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBED_DIM = 64  # final embedding size\n",
    "\n",
    "# Serializable emb layer\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class L2Normalization(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.math.l2_normalize(inputs, axis=1)\n",
    "\n",
    "def build_encoder():\n",
    "    # Input: 128x128 distance matrix\n",
    "    mat_in = Input(shape=(128, 128, 1), name=\"dist_matrix\")\n",
    "\n",
    "    x = layers.Conv2D(32, (5,5), activation=\"relu\", padding=\"same\")(mat_in)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.GlobalMaxPooling2D()(x)\n",
    "    mat_feat = layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "    emb = layers.Dense(EMBED_DIM, activation=None, name=\"embedding\")(mat_feat)\n",
    "    emb = L2Normalization()(emb)\n",
    "\n",
    "    return models.Model(mat_in, emb, name=\"gesture_encoder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607db10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese(encoder):\n",
    "    matA = Input(shape=(128, 128, 1), name=\"matrix_A\")\n",
    "    matB = Input(shape=(128, 128, 1), name=\"matrix_B\")\n",
    "\n",
    "    embA = encoder(matA)\n",
    "    embB = encoder(matB)\n",
    "\n",
    "    dist = layers.Lambda(\n",
    "        lambda x: tf.sqrt(tf.reduce_sum(tf.square(x[0] - x[1]), axis=1, keepdims=True))\n",
    "    )([embA, embB])\n",
    "\n",
    "    return models.Model([matA, matB], dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63bc1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    \"\"\"\n",
    "    y_true: 1 if same class, 0 if different\n",
    "    y_pred: distance between embeddings\n",
    "    \"\"\"\n",
    "    squared = tf.square(y_pred)\n",
    "    margin_squared = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * squared + (1 - y_true) * margin_squared)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dde0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def make_pairs(matrices, labels, batch_size=8):\n",
    "    \"\"\"\n",
    "    Yield batches of distance matrix pairs and targets.\n",
    "    \"\"\"\n",
    "    num_samples = len(matrices)\n",
    "    \n",
    "    # Pre-group indices by class for fast positive sampling\n",
    "    class_to_idxs = {}\n",
    "    for idx, c in enumerate(labels):\n",
    "        class_to_idxs.setdefault(c, []).append(idx)\n",
    "\n",
    "    while True:\n",
    "        matA_batch, matB_batch, y_batch = [], [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            anchor_idx = random.randrange(num_samples)\n",
    "            anchor_label = labels[anchor_idx]\n",
    "\n",
    "            # Positive pair 50%\n",
    "            if random.random() < 0.5:\n",
    "                pos_idx = random.choice(class_to_idxs[anchor_label])\n",
    "                while pos_idx == anchor_idx:\n",
    "                    pos_idx = random.choice(class_to_idxs[anchor_label])\n",
    "                matA_batch.append(matrices[anchor_idx])\n",
    "                matB_batch.append(matrices[pos_idx])\n",
    "                y_batch.append(1.0)\n",
    "            # Negative pair 50%\n",
    "            else:\n",
    "                neg_label = random.choice([l for l in class_to_idxs.keys() if l != anchor_label])\n",
    "                neg_idx = random.choice(class_to_idxs[neg_label])\n",
    "                matA_batch.append(matrices[anchor_idx])\n",
    "                matB_batch.append(matrices[neg_idx])\n",
    "                y_batch.append(0.0)\n",
    "\n",
    "        yield (\n",
    "            (np.array(matA_batch), np.array(matB_batch)),\n",
    "            np.array(y_batch).reshape(-1, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97d42b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gdyer\\anaconda3\\envs\\XR_Gesture\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = build_encoder()\n",
    "siamese = build_siamese(encoder)\n",
    "\n",
    "siamese.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=contrastive_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b214a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('gesture_dataset.npz', allow_pickle=True)  # must allow pickle for object arrays\n",
    "X = data['X']  # gestures\n",
    "y = data['y']  # labels\n",
    "\n",
    "class_to_label = data['class_to_label'].item()  # convert from 0-d object to dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6277f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.3037\n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2150\n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1608\n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2074\n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1249\n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1826\n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1687\n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1290\n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0811\n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1099\n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1094\n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0936\n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0785\n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1268\n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0961\n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0731\n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0581\n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0463\n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0413\n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0541\n"
     ]
    }
   ],
   "source": [
    "def compute_distance_matrix(points):\n",
    "    \"\"\"\n",
    "    points: (num_points, 3)\n",
    "    returns: (num_points, num_points) distance matrix\n",
    "    \"\"\"\n",
    "    points = np.asarray(points, dtype=np.float32)\n",
    "    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "    dist_matrix = np.linalg.norm(diff, axis=-1)\n",
    "    return dist_matrix\n",
    "\n",
    "# Conv to np arrays\n",
    "X = np.array([np.array(p, dtype=np.float32) for p in X]) # Shape should be (128, 64, 3)\n",
    "y = np.array(y)\n",
    "\n",
    "# Build distance matrices for all gestures\n",
    "matrices = np.array([compute_distance_matrix(p) for p in X])\n",
    "matrices = matrices[..., np.newaxis]  # add channel dimension for Conv2D\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "steps_per_epoch = max(1, len(matrices) // batch_size)\n",
    "epochs = 20\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generator_fn():\n",
    "    return make_pairs(matrices, y, batch_size)\n",
    "# Use TF dataset, mostly to avoid restructuring other code to use numpy types\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    generator_fn,\n",
    "    output_signature=(\n",
    "        (\n",
    "            tf.TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train using the TF Dataset\n",
    "history = siamese.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff0f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save(\"gesture_encoder_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba15a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XR_Gesture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
